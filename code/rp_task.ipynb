{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14f23ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#*****************************************************************************************************\n",
    "# Fix settings concerning the ansatz, optimisation and backend\n",
    "#*****************************************************************************************************\n",
    "from time import time \n",
    "import pickle \n",
    "import numpy as np\n",
    "\n",
    "from discopy import Ty, Id, Box, Diagram, Word\n",
    "from discopy.rigid import Cup, Cap, Functor, Swap\n",
    "from discopy.quantum.circuit import bit, qubit\n",
    "from discopy.quantum import Measure\n",
    "from discopy.quantum.tk import to_tk\n",
    "from discopy.quantum.tk import Circuit as tk_Circuit_inDCP\n",
    "\n",
    "from qiskit import IBMQ\n",
    "from pytket.extensions.qiskit import AerBackend, IBMQBackend, IBMQEmulatorBackend\n",
    "from pytket import Circuit as tk_Circuit\n",
    "\n",
    "#-----------------------------\n",
    "# atomic pregroup grammar types\n",
    "#-----------------------------\n",
    "s, n = Ty('S'), Ty('N')\n",
    "\n",
    "#----------------------------------------\n",
    "# settings concerning the ansaetze\n",
    "#----------------------------------------\n",
    "q_s = 0        # number of qubits for sentence type s\n",
    "q_n = 1        # number of qubits for noun type n\n",
    "depth = 2      # number of IQP layers for non-single-qubit words\n",
    "p_n = 1        # number of parameters for a single-qubit word (noun); valued in {1,2,3}.\n",
    "\n",
    "#----------------------------------------\n",
    "# Parameters concerning the optimisation\n",
    "#----------------------------------------\n",
    "n_runs = 1       # number of runs over training procedure\n",
    "niter  = 100     # number of iterations for any optimisation run of training.\n",
    "\n",
    "#----------------------------------------\n",
    "# Parameters for quantum computation\n",
    "#----------------------------------------\n",
    "max_n_shots = 2 ** 13  # maximum shots possible\n",
    "\n",
    "#---------------------\n",
    "# Fix the backend\n",
    "#---------------------\n",
    "backend = AerBackend()  # this is a noise free quantum simulation that will be carried out on your computer\n",
    "                        # and which does not rely on an IBMQ account.\n",
    "\n",
    "# Alternatively: \n",
    "# ***************      !!! Note: Insert here your IBMQ account token !!!\n",
    "#provider = IBMQ.enable_account(<INSERT_IBM_QUANTUM_EXPERIENCE_TOKEN>)\n",
    "\n",
    "#backend = IBMQEmulatorBackend(<backend_name>, <credentials>)\n",
    "#or\n",
    "#backend = IBMQBackend(<backend_name>, <credentials>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018ba709",
   "metadata": {},
   "outputs": [],
   "source": [
    "#******************************************\n",
    "# Data import\n",
    "#******************************************\n",
    "\n",
    "# import train and test datasets: the data entries are all strings of the form 'label sentence' \n",
    "# with the label in {0,1} and with the sentence of the form \"word1_POStag1 word2_POStag2 ...\"\n",
    "\n",
    "with open('../datasets/rp_train_data.txt') as f:\n",
    "    training_data_raw = f.readlines()\n",
    "\n",
    "with open('../datasets/rp_test_data.txt') as f:\n",
    "    testing_data_raw = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd75b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#***************************************************************\n",
    "# Turn the raw input data into data structures convenient below\n",
    "#***************************************************************\n",
    "\n",
    "vocab_dict = dict()     # dictionary to be filled with the vocabulary in the form { word : POStag }\n",
    "data = dict()           # dictionary to be filled with all the data (from train and test subsets); entries of the \n",
    "                        # form { sentence : label } with label encoding '1' as [1.0, 0.0] and '0' as [0.0, 1.0].\n",
    "training_data = []      # list of sentences in the train dataset as strings \"word1 word2 ...\"\n",
    "testing_data = []       # list of sentences in the test dataset as strings \"word1 word2 ...\"\n",
    "\n",
    "# Go through the train data\n",
    "for sent in training_data_raw:\n",
    "    words = sent[2:].split() \n",
    "    sent_untagged = ''\n",
    "    for word in words:\n",
    "        word_untagged, tag = word.split('_')\n",
    "        vocab_dict[word_untagged] = tag\n",
    "        sent_untagged += word_untagged + ' '\n",
    "    sentence = sent_untagged[:-1]\n",
    "    training_data.append(sentence)\n",
    "    label = [1.0, 0.0] if sent[0] == '1' else [0.0, 1.0]\n",
    "    data[sentence] = label\n",
    "\n",
    "# Go through the test data\n",
    "for sent in testing_data_raw:\n",
    "    words = sent[2:].split() \n",
    "    sent_untagged = ''\n",
    "    for word in words:\n",
    "        word_untagged, tag = word.split('_')\n",
    "        vocab_dict[word_untagged] = tag\n",
    "        sent_untagged += word_untagged + ' '\n",
    "    sentence = sent_untagged[:-1]\n",
    "    testing_data.append(sentence)\n",
    "    label = [1.0, 0.0] if sent[0] == '1' else [0.0, 1.0]\n",
    "    data[sentence] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2f186d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#******************************************\n",
    "# Pregroup grammar level\n",
    "#******************************************\n",
    "\n",
    "# The needed composite types of words\n",
    "tv = n.r @ s @ n.l                          # type of transitive verbs\n",
    "that_subj_type = n.r @ n @ s.l @ n          # type of a relative pronoun as a subject\n",
    "that_obj_type = n.r @ n @ (n.l).l @ s.l     # type of a relative pronoun as an object\n",
    "\n",
    "# The two needed pregroup grammar wirings \n",
    "#---------------------------------\n",
    "grammar_subj = (Cup(n, n.r) @ Id(n) @ Id(s.l) @ Cup(n, n.r) @ Id(s) @ Cup(n.l, n)) >> (Id(n) @ Cup(s.l, s))\n",
    "grammar_obj = (Cup(n, n.r) @ Id(n @ (n.l).l @ s.l) @ Cup(n, n.r) @ Id(s @ n.l)) >> (Id(n @ (n.l).l) @ Cup(s.l, s) @ Id(n.l)) >> (Id(n) @ Cup((n.l).l, n.l))\n",
    "\n",
    "# The pregroup vocabulary \n",
    "#------------------------\n",
    "# (note we need the noun effects with type n.l as well as n.r) \n",
    "vocab_dict_pg = dict()\n",
    "vocab_dict_pg_psr = dict()\n",
    "vocab_dict_pg_psr_nl = dict()\n",
    "for word in vocab_dict:\n",
    "    if vocab_dict[word] == 'N':\n",
    "        vocab_dict_pg.update({word: Word(word, n)})\n",
    "        vocab_dict_pg_psr.update({word: Box(word, n.r, Ty())})\n",
    "        vocab_dict_pg_psr_nl.update({word: Box(word, n.l, Ty())})\n",
    "    if vocab_dict[word] == 'V':\n",
    "        vocab_dict_pg.update({word: Word(word, tv)})\n",
    "        vocab_dict_pg_psr.update({word: Word(word, tv)})\n",
    "vocab = list(vocab_dict_pg.values())\n",
    "vocab_psr = list(vocab_dict_pg_psr.values())\n",
    "\n",
    "# The corresponding two kinds of 'that'\n",
    "that_subj = Word('that', that_subj_type)\n",
    "that_obj = Word('that', that_obj_type)\n",
    "        \n",
    "# Go through data and create the diagrams for all phrases and fill into data_pg, \n",
    "# while filling data_pg_psr with the phrases where the nouns have been 'bent around'.\n",
    "data_pg = dict()\n",
    "data_pg_psr = dict()\n",
    "for sentstr in data:\n",
    "    word_list = sentstr.split()\n",
    "    if data[sentstr] == [0.0, 1.0]:   # subject case\n",
    "        sent = (vocab_dict_pg[word_list[0]] @ that_subj @ vocab_dict_pg[word_list[2]] @ vocab_dict_pg[word_list[3]]) >> grammar_subj\n",
    "        sent_psr = (Cap(n.r, n) @ Cap(n, n.l)) >> (vocab_dict_pg_psr[word_list[0]] @ Id(n) @ that_subj @ vocab_dict_pg[word_list[2]] @ Id(n) @ vocab_dict_pg_psr_nl[word_list[3]]) >> grammar_subj\n",
    "    if data[sentstr] == [1.0, 0.0]:   # object case\n",
    "        sent = (vocab_dict_pg[word_list[0]] @ that_obj @ vocab_dict_pg[word_list[2]] @ vocab_dict_pg[word_list[3]]) >> grammar_obj\n",
    "        sent_psr = (Cap(n.r, n) @ Cap(n.r, n)) >> (vocab_dict_pg_psr[word_list[0]] @ Id(n) @ that_obj @ vocab_dict_pg_psr[word_list[2]] @ Id(n) @ vocab_dict_pg[word_list[3]]) >> grammar_obj\n",
    "    data_pg.update({sentstr: sent})\n",
    "    data_pg_psr.update({sentstr: sent_psr.normal_form()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9a7b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#************************************************************************\n",
    "# Two example sentences to see the two different cases diagrammatically:\n",
    "#************************************************************************\n",
    "\n",
    "print('----------------------------------------------')\n",
    "print('Ordinary DisCoCat diagrams')\n",
    "print('----------------------------------------------')\n",
    "print('This is the subject case ([0.0, 1.0]):')\n",
    "data_pg['organization that establish church'].draw()\n",
    "print('This is the object case ([1.0, 0.0]):')\n",
    "data_pg['organization that church establish'].draw()\n",
    "print('----------------------------------------------')\n",
    "print('The DisCoCat diagrams with nouns bent around')\n",
    "print('----------------------------------------------')\n",
    "print('This is the subject case ([0.0, 1.0]):')\n",
    "data_pg_psr['organization that establish church'].draw()\n",
    "print('This is the object case ([1.0, 0.0]):')\n",
    "data_pg_psr['organization that church establish'].draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78dca105",
   "metadata": {},
   "outputs": [],
   "source": [
    "#*****************************************************\n",
    "# Translation to quantum circuits\n",
    "#*****************************************************\n",
    "from discopy.quantum import Ket, IQPansatz, Bra\n",
    "from discopy.quantum.gates import sqrt, H, CZ, Rz, Rx, CX\n",
    "from discopy.quantum.circuit import Id\n",
    "from discopy import CircuitFunctor \n",
    "from discopy.quantum.circuit import Circuit as DCP_Circuit\n",
    "\n",
    "ob = {s: q_s, n: q_n}                           # assignment of number of qubits to atomic grammatical types\n",
    "ob_cqmap = {s: qubit ** q_s, n: qubit ** q_n}   # the form in which it is needed for discopy's cqmap module\n",
    "\n",
    "#-----------------------------------------\n",
    "# parametrised part of ansaetze\n",
    "#-----------------------------------------\n",
    "\n",
    "def single_qubit_iqp_ansatz(params):\n",
    "    if len(params) == 1:\n",
    "        return Rx(params[0])  \n",
    "    if len(params) == 2:\n",
    "        return Rx(params[0]) >> Rz(params[1])\n",
    "    if len(params) == 3:\n",
    "        return IQPansatz(1, params)       \n",
    "\n",
    "def ansatz_state(state, params):  \n",
    "    arity = sum(ob[Ty(factor.name)] for factor in state.cod)\n",
    "    if arity == 1:\n",
    "        return Ket(0) >> single_qubit_iqp_ansatz(params)\n",
    "    else:\n",
    "        return Ket(*tuple([0 for i in range(arity)])) >> IQPansatz(arity, params)\n",
    "    \n",
    "def ansatz_effect(effect, params):  \n",
    "    arity = sum(ob[Ty(factor.name)] for factor in effect.dom)\n",
    "    if arity == 1:\n",
    "        return single_qubit_iqp_ansatz(params) >> Bra(0)\n",
    "    else:\n",
    "        return IQPansatz(arity, params) >> Bra(*tuple([0 for i in range(arity)]))\n",
    "       \n",
    "def ansatz(box, params):\n",
    "    dom_type = box.dom\n",
    "    cod_type = box.cod\n",
    "    if len(dom_type) == 0 and len(cod_type) != 0:\n",
    "        return ansatz_state(box, params)\n",
    "    if len(dom_type) != 0 and len(cod_type) == 0:\n",
    "        return ansatz_effect(box, params)\n",
    "\n",
    "#-----------------------------------------\n",
    "# unparametrised part of ansaetze\n",
    "#-----------------------------------------\n",
    "\n",
    "def relpron_ansatz(s_o_case):                     # relpron modelled as a frobenius spider in a state\n",
    "    GHZ = sqrt(2) @ Ket(0, 0, 0) >> H @ H @ H >> CZ @ Id(1) >> Id(1) @ CZ >> H @ Id(1) @ H\n",
    "    plus_state = Ket(*[0 for i in range(ob[s])])  # on s type wire\n",
    "    if ob[s] != 0:                                # for 1-dim space (0 qubits) there is only one normalised states.\n",
    "        gate = H\n",
    "        for i in range(1, ob[s]): \n",
    "            gate = gate @ H    \n",
    "        plus_state = plus_state >> gate\n",
    "    circ = GHZ\n",
    "    for i in range(1, ob[n]):\n",
    "        circ = circ @ GHZ\n",
    "    circ = circ >> DCP_Circuit.permutation([x + 3*y for x in range(3) for y in range(ob[n])])\n",
    "    if s_o_case == 'subj':\n",
    "        return circ >> Id(ob_cqmap[n] @ ob_cqmap[n]) @ plus_state @ Id(ob_cqmap[n])\n",
    "    if s_o_case == 'obj':\n",
    "        return circ @ plus_state\n",
    "    \n",
    "#----------------------------------------------------------\n",
    "# Define parametrised functor to quantum circuits\n",
    "#----------------------------------------------------------\n",
    "\n",
    "def F(params): \n",
    "    ar = dict()\n",
    "    # parametrised morphisms\n",
    "    for i in range(len(vocab_psr)):\n",
    "        pgbox = vocab_psr[i]\n",
    "        qbox = ansatz(vocab_psr[i], params[i])\n",
    "        ar.update({pgbox: qbox})\n",
    "        if pgbox.cod == Ty():\n",
    "            ar.update({Box(pgbox.name, n.l, Ty()): qbox})\n",
    "    # unparametrised morphisms\n",
    "    ar.update({that_subj: relpron_ansatz('subj'), that_obj: relpron_ansatz('obj')})\n",
    "    return CircuitFunctor(ob_cqmap, ar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f439b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#*****************************************************\n",
    "# The functions to deal with the parametrisation\n",
    "#*****************************************************\n",
    "\n",
    "def param_shapes(vocab_psr):\n",
    "    parshapes = []    \n",
    "    for box in vocab_psr:\n",
    "        dom_type = box.dom\n",
    "        cod_type = box.cod\n",
    "        dom_arity = sum(ob[Ty(factor.name)] for factor in box.dom)\n",
    "        cod_arity = sum(ob[Ty(factor.name)] for factor in box.cod)\n",
    "        if dom_arity == 0 or cod_arity == 0:  # states and effects\n",
    "            arity = max(dom_arity, cod_arity)\n",
    "            assert arity != 0\n",
    "            if arity == 1:\n",
    "                parshapes.append((p_n,))       \n",
    "            if arity != 1:\n",
    "                parshapes.append((depth, arity-1))\n",
    "    return parshapes\n",
    "\n",
    "def rand_params(par_shapes):\n",
    "    params = np.array([]) \n",
    "    for i in range(len(par_shapes)):\n",
    "         params = np.concatenate((params, np.ravel(np.random.rand(*par_shapes[i]))))\n",
    "    return params \n",
    "\n",
    "def reshape_params(unshaped_pars, par_shapes):\n",
    "    pars_reshaped = [[] for ii in range(len(par_shapes))]\n",
    "    shift = 0\n",
    "    for ss, s in enumerate(par_shapes):\n",
    "        idx0 = 0 + shift\n",
    "        if len(s) == 1:\n",
    "            idx1 = s[0] + shift\n",
    "        elif len(s) == 2:\n",
    "            idx1 = s[0] * s[1] + shift\n",
    "        pars_reshaped[ss] = np.reshape(unshaped_pars[idx0:idx1], s)\n",
    "        if len(s) == 1:\n",
    "            shift += s[0]\n",
    "        elif len(s) == 2:\n",
    "            shift += s[0] * s[1]\n",
    "    return pars_reshaped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4f8555",
   "metadata": {},
   "outputs": [],
   "source": [
    "#****************************************\n",
    "# The parameters of the current model\n",
    "#****************************************\n",
    "\n",
    "par_shapes = param_shapes(vocab_psr)\n",
    "rand_unshaped_pars = rand_params(par_shapes)\n",
    "rand_shaped_pars = reshape_params(rand_unshaped_pars, par_shapes)\n",
    "\n",
    "print('Number of parameters:    ', len(rand_unshaped_pars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9288260d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#**************************************************************\n",
    "# Quantum circuit diagrams for 'that' ansaetze and example sentences \n",
    "#**************************************************************\n",
    "\n",
    "func = F(rand_shaped_pars)\n",
    "\n",
    "print(\"The ansatz for 'that' in subject case:\")\n",
    "relpron_ansatz('subj').draw()\n",
    "\n",
    "print(\"The ansatz for 'that' in object case:\")\n",
    "relpron_ansatz('obj').draw()\n",
    "\n",
    "print('Example diagram from above for the subject case mapped under F:')\n",
    "func(data_pg_psr['organization that establish church']).draw(draw_box_labels=True, figsize=(7, 7))\n",
    "\n",
    "print('Example diagram from above for the object case mapped under F:')\n",
    "func(data_pg_psr['organization that church establish']).draw(draw_box_labels=True, figsize=(7, 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54cec0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#********************************************************************************************\n",
    "# Encode data such that the circuits (for one call of cost function etc.) can be sent as one\n",
    "# job to quantum hardware.\n",
    "#********************************************************************************************\n",
    "\n",
    "train_labels = []\n",
    "train_circuits_pg_psr = []\n",
    "for sentstr in training_data:\n",
    "    train_circuits_pg_psr.append(data_pg_psr[sentstr])\n",
    "    train_labels.append(np.array(data[sentstr]))\n",
    "train_labels = np.array(train_labels)\n",
    "\n",
    "test_labels = []\n",
    "test_circuits_pg_psr = []\n",
    "for sentstr in testing_data:\n",
    "    test_circuits_pg_psr.append(data_pg_psr[sentstr])\n",
    "    test_labels.append(np.array(data[sentstr]))\n",
    "test_labels = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0caec6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#**********************************************************************************\n",
    "# Split cost and error functions for time efficiency\n",
    "#**********************************************************************************\n",
    "\n",
    "def get_probs(unshaped_params):\n",
    "    func = F(reshape_params(unshaped_params, par_shapes))\n",
    "    train_circuits = [(func(circ) >> Measure()) for circ in train_circuits_pg_psr]\n",
    "    results = DCP_Circuit.eval(*train_circuits, backend=backend, n_shots=max_n_shots, compilation=backend.default_compilation_pass(2))\n",
    "    results_tweaked = [np.abs(np.array(res.array) - 1e-9) for res in results]\n",
    "    pred_labels_distrs = [res / np.sum(res) for res in results_tweaked]\n",
    "    return pred_labels_distrs\n",
    "\n",
    "def get_cost(pred_labels_distrs):\n",
    "    cross_entropies = np.array([np.sum(train_labels[s] * np.log2(pred_labels_distrs[s])) for s in range(len(train_labels))])\n",
    "    return -1 / len(training_data) * np.sum(cross_entropies)\n",
    "\n",
    "def get_train_error(pred_labels_distrs):\n",
    "    error = 0.0\n",
    "    assert len(pred_labels_distrs[0]) == 2  # rounding only makes sense if labels are binary tuples\n",
    "    pred_labels = [np.round(res) for res in pred_labels_distrs]\n",
    "    for i in range(len(pred_labels)):\n",
    "        if np.sum(pred_labels[i]) != 1.0:  # when equal weights or no counts gives label [1,1] (due to - 1e-9)\n",
    "            error += 1\n",
    "        else:\n",
    "            error += np.abs(train_labels[i][0] - pred_labels[i][0]) # above ensures predicted label as [0,1] or [1,0]\n",
    "    return round(error * 100 / len(training_data), 1)\n",
    "\n",
    "def get_test_error(unshaped_params):\n",
    "    func = F(reshape_params(unshaped_params, par_shapes))\n",
    "    test_circuits = [(func(circ) >> Measure()) for circ in test_circuits_pg_psr]\n",
    "    results = DCP_Circuit.eval(*test_circuits, backend=backend, n_shots=max_n_shots, compilation=backend.default_compilation_pass(2))\n",
    "    results_tweaked = [np.abs(np.array(res.array) - 1e-9) for res in results]\n",
    "    assert len(results_tweaked[0]) == 2\n",
    "    pred_labels = [np.round(res / np.sum(res)) for res in results_tweaked]\n",
    "    error = 0.0\n",
    "    for i in range(len(pred_labels)):\n",
    "        if np.sum(pred_labels[i]) != 1.0:\n",
    "            error += 1\n",
    "        else:\n",
    "            error += np.abs(test_labels[i][0] - pred_labels[i][0])\n",
    "    error = round(error * 100 / len(testing_data), 1)\n",
    "    return error, pred_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693d6df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#**********************************************************************************\n",
    "# Minimization algorithm\n",
    "#**********************************************************************************\n",
    "\n",
    "# This is building on the minimizeSPSA function from the noisyopt package (https://github.com/andim/noisyopt);\n",
    "# here only adjusted for our purposes\n",
    "\n",
    "def my_spsa(get_probs, get_cost, get_train_error, get_test_error, x0,\n",
    "            bounds=None, niter=100, a=1.0, c=1.0, alpha=0.602, gamma=0.101,\n",
    "            print_iter=False, correct_func_value=True,\n",
    "            filename='spsa_output', iters_selected=[]):\n",
    "    A = 0.01 * niter\n",
    "    N = len(x0)\n",
    "    if bounds is None:\n",
    "        project = lambda x: x\n",
    "    else:\n",
    "        bounds = np.asarray(bounds)\n",
    "        project = lambda x: np.clip(x, bounds[:, 0], bounds[:, 1])    \n",
    "    param_history = []\n",
    "    func_history = []\n",
    "    error_history = []\n",
    "    pred_label_history = []\n",
    "    pred_labels_test_error = dict()\n",
    "    test_error_list = []\n",
    "    x = x0    \n",
    "    \n",
    "    # Loop over iterations\n",
    "    for k in range(niter):\n",
    "        if print_iter:\n",
    "            print('-------------', '\\n', 'iteration: ', k, sep='')\n",
    "        start = time()\n",
    "        \n",
    "        # determine stepping parameters\n",
    "        ak = a/(k+1.0+A)**alpha\n",
    "        ck = c/(k+1.0)**gamma\n",
    "        delta = np.random.choice([-1, 1], size=N)\n",
    "        \n",
    "        # move in + direction from previous x\n",
    "        xplus = project(x + ck*delta)        \n",
    "        if print_iter:\n",
    "            print('Call for xplus')\n",
    "        results_tweaked_plus = get_probs(xplus)\n",
    "        funcplus = get_cost(results_tweaked_plus)\n",
    "        \n",
    "        # move in - direction from previous x\n",
    "        xminus = project(x - ck * delta)\n",
    "        if print_iter:\n",
    "            print('Call for xminus')\n",
    "        results_tweaked_minus = get_probs(xminus)\n",
    "        funcminus = get_cost(results_tweaked_minus)\n",
    "        \n",
    "        # new step\n",
    "        grad = (funcplus - funcminus) / (xplus-xminus)\n",
    "        x = project(x - ak*grad)\n",
    "        param_history.append(x)\n",
    "        \n",
    "        # determine current func and error\n",
    "        if correct_func_value or k == (niter - 1):  # In order to save time the cost at x is only evaluated for final step\n",
    "            if print_iter:\n",
    "                print('Call for current_func_value')\n",
    "            results_tweaked = get_probs(x)\n",
    "            current_func_value = get_cost(results_tweaked)\n",
    "            error = get_train_error(results_tweaked)\n",
    "            pred_label_history.append(results_tweaked)\n",
    "        else:\n",
    "            current_func_value = funcplus\n",
    "            error = get_train_error(results_tweaked_plus)\n",
    "            pred_label_history.append(results_tweaked_plus)\n",
    "        \n",
    "        # calculate test error if a 'selected iteration' \n",
    "        if k in iters_selected:\n",
    "            print('Calculate test error for iteration:', k)\n",
    "            res = get_test_error(x)\n",
    "            test_error_list.append(res[0])\n",
    "            pred_labels_test_error.update({k: res[1]})\n",
    "        \n",
    "        func_history.append(current_func_value)\n",
    "        error_history.append(error)\n",
    "\n",
    "        # save to file\n",
    "        dump_data = {\n",
    "            'param_history': param_history,\n",
    "            'func_history': func_history,\n",
    "            'error_history': error_history,\n",
    "            'pred_label_history': pred_label_history,\n",
    "            'iters_selected': iters_selected,\n",
    "            'test_error_list': test_error_list,\n",
    "            'pred_labels_test_error': pred_labels_test_error\n",
    "        }\n",
    "        with open(filename+'.pickle', 'wb') as file_handle:\n",
    "            pickle.dump(dump_data, file_handle)\n",
    "        \n",
    "        if print_iter:\n",
    "            print('Time taken for this iteration: ', time() - start)\n",
    "    return param_history, func_history, error_history, test_error_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed0009d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#**********************************************************\n",
    "# Quantum run: training and error calculation\n",
    "#**********************************************************\n",
    "\n",
    "bounds = [[0.0, 1.0] for ii in range(len(rand_unshaped_pars))]\n",
    "c_fix = 0.1   \n",
    "a_est = 0.07272  \n",
    "\n",
    "param_histories = []\n",
    "cost_histories = np.zeros((n_runs, niter))\n",
    "error_train_histories = np.zeros((n_runs, niter))\n",
    "\n",
    "# For test error calculation (for reasons of time cost not for all iterations)\n",
    "iters_selected = [(i+1)*10-1 for i in range(int(niter/10))]\n",
    "iters_selected.insert(0, 0)\n",
    "error_test_histories = np.zeros((n_runs, len(iters_selected)))\n",
    "\n",
    "for i in range(n_runs):\n",
    "    print('---------------------------------')\n",
    "    print('Start run ', i+1)\n",
    "    rand_unshaped_pars = rand_params(par_shapes)\n",
    "    start = time()\n",
    "    res = my_spsa(get_probs, get_cost, get_train_error, get_test_error, rand_unshaped_pars,\n",
    "                  bounds=bounds, niter=niter, a=a_est, c=c_fix,\n",
    "                  print_iter=True, correct_func_value=False, filename=('RP_task_SPSAOutput_Run' + str(i)),\n",
    "                  iters_selected=iters_selected)\n",
    "    param_histories.append(res[0])   \n",
    "    cost_histories[i, :] = res[1]\n",
    "    error_train_histories[i, :] = res[2]\n",
    "    error_test_histories[i, :] = res[3]\n",
    "    print('run', i+1, 'done')\n",
    "    print('Time taken: ', time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273ee781",
   "metadata": {},
   "outputs": [],
   "source": [
    "#****************************************************\n",
    "# Averaging\n",
    "#****************************************************\n",
    "\n",
    "# In case N_runs > 1, one may want to calculate cost and errors averaged over several runs...\n",
    "\n",
    "# In this example notebook however not done, hence:\n",
    "cost_history = cost_histories[0, :]\n",
    "error_train_history = error_train_histories[0, :]\n",
    "error_test_history = error_test_histories[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67b7611",
   "metadata": {},
   "outputs": [],
   "source": [
    "#****************************************************\n",
    "# Summary plot\n",
    "#****************************************************\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(13, 8))\n",
    "\n",
    "ax1.plot(range(len(cost_history)), cost_history, '-k', markersize=4, label='cost')\n",
    "ax1.set_ylabel(r\"Cost\", fontsize='x-large')\n",
    "ax1.set_xlabel(r\"SPSA~iterations\", fontsize='x-large')\n",
    "ax1.legend(loc='upper center', fontsize='x-large')\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.set_ylabel(r\"Error in \\%\", fontsize='x-large')\n",
    "ax2.plot(range(len(error_train_history)), error_train_history, '-g', markersize=4, label='training error')\n",
    "ax2.plot(iters_selected, error_test_history, 'xb', markersize=7, label='testing error')\n",
    "ax2.legend(loc='upper right', fontsize='x-large')\n",
    "\n",
    "plt.title('RP task, quantum run -- results', fontsize='x-large')\n",
    "plt.savefig('RP_task_Results.png', dpi=300, facecolor='white')  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc764e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
