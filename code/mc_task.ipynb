{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fa2703",
   "metadata": {},
   "outputs": [],
   "source": [
    "#*****************************************************************************************************\n",
    "# Fix settings concerning the ansatz, optimisation and backend\n",
    "#*****************************************************************************************************\n",
    "\n",
    "from time import time \n",
    "import pickle \n",
    "import numpy as np\n",
    "\n",
    "from discopy import Ty, Id, Box, Diagram, Word\n",
    "from discopy.rigid import Cup, Cap, Functor, Swap\n",
    "from discopy.quantum.circuit import bit, qubit\n",
    "from discopy.quantum import Measure\n",
    "from discopy.quantum.tk import to_tk\n",
    "from discopy.quantum.tk import Circuit as tk_Circuit_inDCP\n",
    "\n",
    "from qiskit import IBMQ\n",
    "from pytket.extensions.qiskit import AerBackend, IBMQBackend, IBMQEmulatorBackend\n",
    "from pytket import Circuit as tk_Circuit\n",
    "\n",
    "#-----------------------------\n",
    "# atomic pregroup grammar types\n",
    "#-----------------------------\n",
    "s, n = Ty('S'), Ty('N')\n",
    "\n",
    "#----------------------------------------\n",
    "# settings concerning the ansaetze\n",
    "#----------------------------------------\n",
    "q_s = 1        # number of qubits for sentence type s\n",
    "q_n = 1        # number of qubits for noun type n\n",
    "depth = 1      # number of IQP layers for non-single-qubit words\n",
    "p_n = 3        # number of parameters for a single-qubit word (noun); valued in {1,2,3}.\n",
    "\n",
    "#----------------------------------------\n",
    "# Parameters concerning the optimisation\n",
    "#----------------------------------------\n",
    "n_runs = 1      # number of runs over training procedure\n",
    "niter  = 100    # number of iterations for any optimisation run of training.\n",
    "\n",
    "#----------------------------------------\n",
    "# Parameters for quantum computation\n",
    "#----------------------------------------\n",
    "max_n_shots = 2 ** 13  # maximum shots possible\n",
    "\n",
    "#---------------------\n",
    "# Fix the backend\n",
    "#---------------------\n",
    "backend = AerBackend()  # this is a noise free quantum simulation that will be carried out on your computer\n",
    "                        # and which does not rely on an IBMQ account.\n",
    "\n",
    "# Alternatively: \n",
    "# ***************      !!! Note: Insert here your IBMQ account token !!!\n",
    "# provider = IBMQ.enable_account(<INSERT_IBM_QUANTUM_EXPERIENCE_TOKEN>)\n",
    "\n",
    "# backend = IBMQEmulatorBackend(<backend_name>, <credentials>)\n",
    "#or\n",
    "# backend = IBMQBackend(<backend_name>, <credentials>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7465b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#******************************************\n",
    "# Data import\n",
    "#******************************************\n",
    "\n",
    "# import train, dev and test datasets: the data entries are all strings of the form 'label sentence' \n",
    "# with the label in {0,1} and with the sentence of the form \"word1_POStag1 word2_POStag2 ...\"\n",
    "\n",
    "with open('../datasets/mc_train_data.txt') as f:\n",
    "    training_data_raw = f.readlines()\n",
    "\n",
    "with open('../datasets/mc_dev_data.txt') as f:\n",
    "    dev_data_raw = f.readlines()\n",
    "\n",
    "with open('../datasets/mc_test_data.txt') as f:\n",
    "    testing_data_raw = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017bdcb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#***************************************************************\n",
    "# Turn the raw input data into data structures convenient below\n",
    "#***************************************************************\n",
    "\n",
    "vocab = dict()          # dictionary to be filled with the vocabulary in the form { word : POStag }\n",
    "data = dict()           # dictionary to be filled with all the data (train, dev and test subsets); entries of the \n",
    "                        # form { sentence : label } with label encoding '1' as [1.0, 0.0] and '0' as [0.0, 1.0]\n",
    "training_data = []      # list of sentences in the train dataset as strings \"word1 word2 ...\"\n",
    "dev_data = []           # list of sentences in the dev dataset as strings \"word1 word2 ...\"\n",
    "testing_data = []       # list of sentences in the test dataset as strings \"word1 word2 ...\"\n",
    "\n",
    "# Go through the train data\n",
    "for sent in training_data_raw:\n",
    "    words = sent[2:].split() \n",
    "    sent_untagged = ''\n",
    "    for word in words:\n",
    "        word_untagged, tag = word.split('_')\n",
    "        vocab[word_untagged] = tag\n",
    "        sent_untagged += word_untagged + ' '\n",
    "    sentence = sent_untagged[:-1]\n",
    "    training_data.append(sentence)\n",
    "    label = np.array([1.0, 0.0]) if sent[0] == '1' else np.array([0.0, 1.0])\n",
    "    data[sentence] = label\n",
    "\n",
    "# Go through the dev data\n",
    "for sent in dev_data_raw:\n",
    "    words = sent[2:].split() \n",
    "    sent_untagged = ''\n",
    "    for word in words:\n",
    "        word_untagged, tag = word.split('_')\n",
    "        vocab[word_untagged] = tag\n",
    "        sent_untagged += word_untagged + ' '\n",
    "    sentence = sent_untagged[:-1]\n",
    "    dev_data.append(sentence)\n",
    "    label = np.array([1.0, 0.0]) if sent[0] == '1' else np.array([0.0, 1.0])\n",
    "    data[sentence] = label\n",
    "    \n",
    "# Go through the test data\n",
    "for sent in testing_data_raw:\n",
    "    words = sent[2:].split() \n",
    "    sent_untagged = ''\n",
    "    for word in words:\n",
    "        word_untagged, tag = word.split('_')\n",
    "        vocab[word_untagged] = tag\n",
    "        sent_untagged += word_untagged + ' '\n",
    "    sentence = sent_untagged[:-1]\n",
    "    testing_data.append(sentence)\n",
    "    label = np.array([1.0, 0.0]) if sent[0] == '1' else np.array([0.0, 1.0])\n",
    "    data[sentence] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1173de73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#*****************************************************\n",
    "# The sentences as diagrams via CFG production rules\n",
    "#*****************************************************\n",
    "\n",
    "#----------------------------\n",
    "# Further POS tags:\n",
    "#----------------------------\n",
    "nphr, adj, tv, iv, vphr = Ty('NP'), Ty('ADJ'), Ty('TV'), Ty('IV'), Ty('VP')\n",
    "\n",
    "#----------------------------\n",
    "# The vocabulary in DisCoPy\n",
    "#----------------------------\n",
    "vocab_dict_boxes = dict()\n",
    "for word, tag in vocab.items():\n",
    "    if tag == 'N':\n",
    "        vocab_dict_boxes.update({word: Word(word, n)})\n",
    "    if tag == 'TV':\n",
    "        vocab_dict_boxes.update({word: Word(word, tv)})\n",
    "    if tag == 'ADJ':\n",
    "        vocab_dict_boxes.update({word: Word(word, adj)})\n",
    "\n",
    "#-------------------------------------\n",
    "# The CFG production rules as boxes\n",
    "#-------------------------------------\n",
    "r0 = Box('R0', nphr @ vphr, s)\n",
    "r1 = Box('R1', tv @ nphr, vphr)\n",
    "r2 = Box('R2', adj @ n, nphr)\n",
    "r3 = Box('R3', iv, vphr)\n",
    "r4 = Box('R4', n, nphr)\n",
    "\n",
    "#---------------------------------------------\n",
    "# The needed grammatical sentence structures\n",
    "#---------------------------------------------\n",
    "grammar_dict = {\n",
    "    'N_TV_N': ((Id(n @ tv) @ r4) >> (r4 @ r1) >> r0),\n",
    "    'N_TV_ADJ_N': ((Id(n @ tv) @ r2) >> (r4 @ r1) >> r0),\n",
    "    'ADJ_N_TV_N': ((Id(adj @ n @ tv) @ r4) >> (r2 @ r1) >> r0),\n",
    "    'ADJ_N_TV_ADJ_N': ((Id(adj @ n @ tv) @ r2) >> (r2 @ r1) >> r0)\n",
    "}\n",
    "\n",
    "#---------------------------------------------\n",
    "# Create CFG diagrams for the sentences\n",
    "#---------------------------------------------\n",
    "sentences_dict = dict()\n",
    "for sentstr in list(data.keys()):\n",
    "    grammar_id = ''\n",
    "    sentence = Id(Ty())\n",
    "    for word in sentstr.split(' '):\n",
    "        grammar_id += (vocab[word] + '_')\n",
    "        sentence = sentence @ vocab_dict_boxes[word]\n",
    "    grammar_id = grammar_id[:-1]\n",
    "    sentence = sentence >> grammar_dict[grammar_id]\n",
    "    sentences_dict.update({sentstr: [sentence, grammar_id]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7b9365",
   "metadata": {},
   "outputs": [],
   "source": [
    "#***************************************************************\n",
    "# Translation to pregroup grammar \n",
    "#***************************************************************\n",
    "from discopy.grammar.pregroup import draw\n",
    "\n",
    "# From POS tags to Pregroup types:\n",
    "ob_pg = {n: n, s: s, adj: n @ n.l, tv: n.r @ s @ n.l, vphr:  n.r @ s, nphr: n}\n",
    "\n",
    "# From CFG rules to Pregroup reductions: \n",
    "ar_pg = {\n",
    "    r0: Cup(n, n.r) @ Id(s),\n",
    "    r1: Id(n.r @ s) @ Cup(n.l, n),\n",
    "    r2: Id(n) @ Cup(n.l, n),\n",
    "    r3: Id(n.r @ s),\n",
    "    r4: Id(n)\n",
    "}\n",
    "\n",
    "# The vocabulary as DisCoPy boxes with pregroup types\n",
    "vocab_pg = [Word(vocab_dict_boxes[word].name, ob_pg[vocab_dict_boxes[word].cod]) for word in vocab.keys()]\n",
    "\n",
    "# The mapping of morphisms\n",
    "ar_pg.update({vocab_dict_boxes[word]: Word(vocab_dict_boxes[word].name, ob_pg[vocab_dict_boxes[word].cod]) for word in vocab.keys()})\n",
    "\n",
    "# The functor that translates from CFG to pregroup\n",
    "t2p = Functor(ob_pg, ar_pg)\n",
    "\n",
    "sentences_pg_dict = dict()\n",
    "for sentstr in sentences_dict:\n",
    "    sentences_pg_dict.update({sentstr: [t2p(sentences_dict[sentstr][0]), sentences_dict[sentstr][1]]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794e140d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#******************************************************************************************************\n",
    "# (Optional) For visualisation: the sentences as pregroup diagrams -- before 'bending nouns around'\n",
    "#******************************************************************************************************\n",
    "\n",
    "for sentstr in sentences_pg_dict:\n",
    "    sentences_pg_dict[sentstr][0].draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3e5c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#******************************************************\n",
    "# Bending the nouns around\n",
    "#******************************************************\n",
    "sentences_pg_psr_dict = dict()\n",
    "\n",
    "for sentstr in sentences_pg_dict:\n",
    "    grammar_id = sentences_pg_dict[sentstr][1]\n",
    "    num_words = len(grammar_id.split('_'))\n",
    "    words = sentences_pg_dict[sentstr][0][:num_words].boxes\n",
    "    grammar = sentences_pg_dict[sentstr][0][num_words:]\n",
    "    if grammar_id == 'N_TV_N':\n",
    "        noun1 = Box(words[0].name, n.r, Ty())\n",
    "        noun2 = Box(words[2].name, n.l, Ty())\n",
    "        words_new = (Cap(n.r, n) @ Cap(n, n.l)) >> (noun1 @ Id(n) @ words[1] @ Id(n) @ noun2)\n",
    "    if grammar_id == 'ADJ_N_TV_N':\n",
    "        noun1 = Box(words[1].name, n.l, Ty())\n",
    "        noun2 = Box(words[3].name, n.l, Ty())\n",
    "        words_new = (Cap(n, n.l) @ Cap(n, n.l)) >> (words[0] @ Id(n) @ noun1 @ words[2] @ Id(n) @ noun2)\n",
    "    if grammar_id == 'N_TV_ADJ_N':\n",
    "        noun1 = Box(words[0].name, n.r, Ty())\n",
    "        noun2 = Box(words[3].name, n.l, Ty())\n",
    "        words_new = (Cap(n.r, n) @ Cap(n, n.l)) >> (noun1 @ Id(n) @ words[1] @ words[2] @ Id(n) @ noun2)\n",
    "    if grammar_id == 'ADJ_N_TV_ADJ_N':\n",
    "        noun1 = Box(words[1].name, n.l, Ty())\n",
    "        noun2 = Box(words[4].name, n.l, Ty())\n",
    "        words_new = (Cap(n, n.l) @ Cap(n, n.l)) >> (words[0] @ Id(n) @ noun1 @ words[2] @ words[3] @ Id(n) @ noun2)\n",
    "    # add newly wired sentence to dictionary\n",
    "    sentence = words_new >> grammar\n",
    "    # Yank snakes and add to dictionary\n",
    "    sentences_pg_psr_dict.update({sentstr: sentence.normal_form()})\n",
    "\n",
    "# Now for the vocab\n",
    "vocab_psr = []\n",
    "for word in vocab_pg:\n",
    "    if word.cod == Ty('N'):\n",
    "        vocab_psr.append(Box(word.name, n.r, Ty()))   # n.l case is dealt with in definition of quantum functor\n",
    "    else:\n",
    "        vocab_psr.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb1ac90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#******************************************************************************************************\n",
    "# (Optional) For visualisation: the sentences as pregroup diagrams -- after 'bending nouns around'\n",
    "#******************************************************************************************************\n",
    "for sentstr in sentences_pg_psr_dict:\n",
    "    sentences_pg_psr_dict[sentstr].draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8c2300",
   "metadata": {},
   "outputs": [],
   "source": [
    "#*****************************************************\n",
    "# Translation to quantum circuits\n",
    "#*****************************************************\n",
    "from discopy.quantum import Ket, IQPansatz, Bra\n",
    "from discopy.quantum.gates import sqrt, H, CZ, Rz, Rx, CX\n",
    "from discopy.quantum.circuit import Id\n",
    "from discopy import CircuitFunctor\n",
    "from discopy.quantum.circuit import Circuit as DCP_Circuit\n",
    "\n",
    "ob = {s: q_s, n: q_n}                           # assignment of number of qubits to atomic grammatical types\n",
    "ob_cqmap = {s: qubit ** q_s, n: qubit ** q_n}   # the form in which it is needed for discopy's cqmap module\n",
    "\n",
    "#-----------------------------------------\n",
    "# parametrised part of ansaetze\n",
    "#-----------------------------------------\n",
    "\n",
    "def single_qubit_iqp_ansatz(params):\n",
    "    if len(params) == 1:\n",
    "        return Rx(params[0])  \n",
    "    if len(params) == 2:\n",
    "        return Rx(params[0]) >> Rz(params[1])\n",
    "    if len(params) == 3:\n",
    "        return IQPansatz(1, params)       \n",
    "\n",
    "def ansatz_state(state, params):  \n",
    "    arity = sum(ob[Ty(factor.name)] for factor in state.cod)\n",
    "    if arity == 1:\n",
    "        return Ket(0) >> single_qubit_iqp_ansatz(params)\n",
    "    else:\n",
    "        return Ket(*tuple([0 for i in range(arity)])) >> IQPansatz(arity, params)\n",
    "    \n",
    "def ansatz_effect(effect, params):  \n",
    "    arity = sum(ob[Ty(factor.name)] for factor in effect.dom)\n",
    "    if arity == 1:\n",
    "        return single_qubit_iqp_ansatz(params) >> Bra(0)\n",
    "    else:\n",
    "        return IQPansatz(arity, params) >> Bra(*tuple([0 for i in range(arity)]))\n",
    "       \n",
    "def ansatz(box,params):\n",
    "    dom_type = box.dom\n",
    "    cod_type = box.cod\n",
    "    if len(dom_type) == 0 and len(cod_type) != 0:\n",
    "        return ansatz_state(box, params)\n",
    "    if len(dom_type) != 0 and len(cod_type) == 0:\n",
    "        return ansatz_effect(box, params)\n",
    "\n",
    "#----------------------------------------------------------\n",
    "# Define parametrised functor to quantum circuits\n",
    "#----------------------------------------------------------\n",
    "def F(params): \n",
    "    ar = dict()\n",
    "    for i in range(len(vocab_psr)):\n",
    "        pgbox = vocab_psr[i]\n",
    "        qbox = ansatz(vocab_psr[i], params[i])\n",
    "        ar.update({pgbox: qbox})\n",
    "        if pgbox.cod == Ty():\n",
    "            ar.update({Box(pgbox.name, n.l, Ty()): qbox})  # send the effect with n.l to same quantum effect\n",
    "    return CircuitFunctor(ob_cqmap, ar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5099a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#*****************************************************\n",
    "# Functions to deal with the parametrisation\n",
    "#*****************************************************\n",
    "\n",
    "def paramshapes(vocab_psr):\n",
    "    parshapes = []    \n",
    "    for box in vocab_psr:\n",
    "        dom_type = box.dom\n",
    "        cod_type = box.cod\n",
    "        dom_arity = sum(ob[Ty(factor.name)] for factor in box.dom)\n",
    "        cod_arity = sum(ob[Ty(factor.name)] for factor in box.cod)\n",
    "        if dom_arity == 0 or cod_arity == 0:  # states and effects\n",
    "            arity = max(dom_arity, cod_arity)\n",
    "            assert arity != 0\n",
    "            if arity == 1:\n",
    "                parshapes.append((p_n,))       \n",
    "            if arity != 1:\n",
    "                parshapes.append((depth, arity-1))\n",
    "    return parshapes\n",
    "\n",
    "def randparams(par_shapes):\n",
    "    params = np.array([]) \n",
    "    for i in range(len(par_shapes)):\n",
    "        params = np.concatenate((params, np.ravel(np.random.rand(*par_shapes[i]))))\n",
    "    return params \n",
    "\n",
    "def reshape_params(unshaped_pars, par_shapes):\n",
    "    pars_reshaped = [[] for ii in range(len(par_shapes))]\n",
    "    shift = 0\n",
    "    for ss, s in enumerate(par_shapes):\n",
    "        idx0 = 0 + shift\n",
    "        if len(s) == 1:\n",
    "            idx1 = s[0] + shift\n",
    "        elif len(s) == 2:\n",
    "            idx1 = s[0] * s[1] + shift\n",
    "        pars_reshaped[ss] = np.reshape(unshaped_pars[idx0:idx1], s)\n",
    "        if len(s) == 1:\n",
    "            shift += s[0]\n",
    "        elif len(s) == 2:\n",
    "            shift += s[0] * s[1]\n",
    "    return pars_reshaped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9461d3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#****************************************\n",
    "# Parameters of the current model\n",
    "#****************************************\n",
    "\n",
    "par_shapes = paramshapes(vocab_psr)\n",
    "rand_unshaped_pars = randparams(par_shapes)\n",
    "rand_shaped_pars = reshape_params(rand_unshaped_pars, par_shapes)\n",
    "\n",
    "print('Number of parameters:    ', len(rand_unshaped_pars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8127f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#**************************************************************\n",
    "# (Optional) Quantum circuit diagrams for the sentences \n",
    "#**************************************************************\n",
    "\n",
    "func = F(rand_shaped_pars)\n",
    "\n",
    "for sentstr in sentences_pg_psr_dict:\n",
    "    func(sentences_pg_psr_dict[sentstr]).draw(draw_box_labels=True, figsize=(5, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27995040",
   "metadata": {},
   "outputs": [],
   "source": [
    "#********************************************************************************************\n",
    "# Encode data such that the circuits (for one call of cost function etc.) can be sent as one\n",
    "# job to quantum hardware.\n",
    "#********************************************************************************************\n",
    "\n",
    "train_labels = []\n",
    "train_circuits_pg_psr = []\n",
    "for sentstr in training_data:\n",
    "    train_circuits_pg_psr.append(sentences_pg_psr_dict[sentstr])\n",
    "    train_labels.append(np.array(data[sentstr]))\n",
    "train_labels = np.array(train_labels)\n",
    "\n",
    "dev_labels = []\n",
    "dev_circuits_pg_psr = []\n",
    "for sentstr in dev_data:\n",
    "    dev_circuits_pg_psr.append(sentences_pg_psr_dict[sentstr])\n",
    "    dev_labels.append(np.array(data[sentstr]))\n",
    "dev_labels = np.array(dev_labels)\n",
    "\n",
    "test_labels = []\n",
    "test_circuits_pg_psr = []\n",
    "for sentstr in testing_data:\n",
    "    test_circuits_pg_psr.append(sentences_pg_psr_dict[sentstr])\n",
    "    test_labels.append(np.array(data[sentstr]))\n",
    "test_labels = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216855dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#**********************************************************************************\n",
    "# Split cost and error functions for time efficiency\n",
    "#**********************************************************************************\n",
    "\n",
    "def get_probs(unshaped_params):\n",
    "    func = F(reshape_params(unshaped_params, par_shapes))\n",
    "    train_circuits = [(func(circ) >> Measure()) for circ in train_circuits_pg_psr]\n",
    "    results = DCP_Circuit.eval(*train_circuits, backend=backend, n_shots=max_n_shots, compilation=backend.default_compilation_pass(2))\n",
    "    results_tweaked = [np.abs(np.array(res.array) - 1e-9) for res in results]\n",
    "    pred_labels_distrs = [res / np.sum(res) for res in results_tweaked]\n",
    "    return pred_labels_distrs\n",
    "\n",
    "def get_cost(pred_labels_distrs):\n",
    "    cross_entropies = np.array([np.sum(train_labels[s] * np.log2(pred_labels_distrs[s])) for s in range(len(train_labels))])\n",
    "    return -1 / len(training_data) * np.sum(cross_entropies)\n",
    "\n",
    "def get_train_error(pred_labels_distrs):\n",
    "    error = 0.0\n",
    "    assert len(pred_labels_distrs[0]) == 2  # rounding only makes sense if labels are binary tuples\n",
    "    pred_labels = [np.round(res) for res in pred_labels_distrs]\n",
    "    for i in range(len(pred_labels)):\n",
    "        if np.sum(pred_labels[i]) != 1.0:  # when equal weights or no counts gives label [1,1] (due to - 1e-9)\n",
    "            error += 1\n",
    "        else:\n",
    "            error += np.abs(train_labels[i][0] - pred_labels[i][0]) # above ensures precited label as [0,1] or [1,0]\n",
    "    return round(error * 100 / len(training_data), 1)\n",
    "\n",
    "def get_dev_error(unshaped_params):\n",
    "    func = F(reshape_params(unshaped_params, par_shapes))\n",
    "    dev_circuits = [(func(circ) >> Measure()) for circ in dev_circuits_pg_psr]\n",
    "    results = DCP_Circuit.eval(*dev_circuits, backend=backend, n_shots=max_n_shots, compilation=backend.default_compilation_pass(2))\n",
    "    results_tweaked = [np.abs(np.array(res.array) - 1e-9) for res in results]\n",
    "    assert len(results_tweaked[0]) == 2\n",
    "    pred_labels = [np.round(res / np.sum(res)) for res in results_tweaked]\n",
    "    error = 0.0\n",
    "    for i in range(len(pred_labels)):\n",
    "        if np.sum(pred_labels[i]) != 1.0:\n",
    "            error += 1\n",
    "        else:\n",
    "            error += np.abs(dev_labels[i][0] - pred_labels[i][0])\n",
    "    error = round(error * 100 / len(dev_data), 1)\n",
    "    return error, pred_labels\n",
    "\n",
    "def get_test_error(unshaped_params):\n",
    "    func = F(reshape_params(unshaped_params, par_shapes))\n",
    "    test_circuits = [(func(circ) >> Measure()) for circ in test_circuits_pg_psr]\n",
    "    results = DCP_Circuit.eval(*test_circuits, backend=backend, n_shots=max_n_shots, compilation=backend.default_compilation_pass(2))\n",
    "    results_tweaked = [np.abs(np.array(res.array) - 1e-9) for res in results]\n",
    "    assert len(results_tweaked[0]) == 2\n",
    "    pred_labels = [np.round(res / np.sum(res)) for res in results_tweaked]\n",
    "    error = 0.0\n",
    "    for i in range(len(pred_labels)):\n",
    "        if np.sum(pred_labels[i]) != 1.0:\n",
    "            error += 1\n",
    "        else:\n",
    "            error += np.abs(test_labels[i][0] - pred_labels[i][0])\n",
    "    error = round(error * 100 / len(testing_data), 1)\n",
    "    return error, pred_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4966cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#**********************************************************************************\n",
    "# Minimization algorithm\n",
    "#**********************************************************************************\n",
    "\n",
    "# This is building on the minimizeSPSA function from the noisyopt package (https://github.com/andim/noisyopt);\n",
    "# here only adjusted for our purposes. As this is an example notebook for a final run of the experiment\n",
    "# the dev set is not used here.\n",
    "\n",
    "def my_spsa(get_probs, get_cost, get_train_error, get_test_error, x0,\n",
    "            bounds=None, niter=100, a=1.0, c=1.0, alpha=0.602, gamma=0.101,\n",
    "            print_iter=False, correct_func_value=True,\n",
    "            filename='spsa_output', iters_selected=[]):\n",
    "    A = 0.01 * niter\n",
    "    N = len(x0)\n",
    "    if bounds is None:\n",
    "        project = lambda x: x\n",
    "    else:\n",
    "        bounds = np.asarray(bounds)\n",
    "        project = lambda x: np.clip(x, bounds[:, 0], bounds[:, 1])    \n",
    "    param_history = []\n",
    "    func_history = []\n",
    "    error_history = []\n",
    "    pred_label_history = []\n",
    "    pred_labels_test_error = dict()\n",
    "    test_error_list = []\n",
    "    x = x0    \n",
    "    \n",
    "    # Loop over iterations\n",
    "    for k in range(niter):\n",
    "        if print_iter:\n",
    "            print('-------------', '\\n', 'iteration: ', k, sep='')\n",
    "        start = time()\n",
    "        \n",
    "        # determine stepping parameters\n",
    "        ak = a/(k+1.0+A)**alpha\n",
    "        ck = c/(k+1.0)**gamma\n",
    "        delta = np.random.choice([-1, 1], size=N)\n",
    "        \n",
    "        # move in + direction from previous x\n",
    "        xplus = project(x + ck*delta)        \n",
    "        if print_iter:\n",
    "            print('Call for xplus')\n",
    "        results_tweaked_plus = get_probs(xplus)\n",
    "        funcplus = get_cost(results_tweaked_plus)\n",
    "        \n",
    "        # move in - direction from previous x\n",
    "        xminus = project(x - ck*delta)\n",
    "        if print_iter:\n",
    "            print('Call for xminus')\n",
    "        results_tweaked_minus = get_probs(xminus)\n",
    "        funcminus = get_cost(results_tweaked_minus)\n",
    "        \n",
    "        # new step\n",
    "        grad = (funcplus - funcminus) / (xplus-xminus)\n",
    "        x = project(x - ak*grad)\n",
    "        param_history.append(x)\n",
    "        \n",
    "        # determine current func and error\n",
    "        if correct_func_value or k == (niter - 1):  # In order to save time the cost at x is only evaluated for final step\n",
    "            if print_iter:\n",
    "                print('Call for current_func_value')\n",
    "            results_tweaked = get_probs(x)\n",
    "            current_func_value = get_cost(results_tweaked)\n",
    "            error = get_train_error(results_tweaked)\n",
    "            pred_label_history.append(results_tweaked)\n",
    "        else:\n",
    "            current_func_value = funcplus\n",
    "            error = get_train_error(results_tweaked_plus)\n",
    "            pred_label_history.append(results_tweaked_plus)\n",
    "        \n",
    "        # calculate test error if a 'selected iteration' \n",
    "        if k in iters_selected:\n",
    "            print('Calculate test error for iteration:', k)\n",
    "            res = get_test_error(x)\n",
    "            test_error_list.append(res[0])\n",
    "            pred_labels_test_error.update({k: res[1]})\n",
    "        \n",
    "        func_history.append(current_func_value)\n",
    "        error_history.append(error)\n",
    "\n",
    "        # save to file\n",
    "        dump_data = {\n",
    "            'param_history': param_history,\n",
    "            'func_history': func_history,\n",
    "            'error_history': error_history,\n",
    "            'predlabel_history': pred_label_history,\n",
    "            'iters_selected': iters_selected,\n",
    "            'test_error_list': test_error_list,\n",
    "            'pred_labels_test_error': pred_labels_test_error\n",
    "        }\n",
    "        with open(filename+'.pickle', 'wb') as file_handle:\n",
    "            pickle.dump(dump_data, file_handle)\n",
    "        \n",
    "        if print_iter:\n",
    "            print('Time taken for this iteration: ', time() - start)\n",
    "    return param_history, func_history, error_history, test_error_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51d342a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#************************************\n",
    "# Quantum run: training and error calculation\n",
    "#************************************\n",
    "\n",
    "bounds = [[0.0, 1.0] for ii in range(len(rand_unshaped_pars))]\n",
    "c_fix = 0.1   \n",
    "a_est = 0.015  \n",
    "\n",
    "param_histories = []\n",
    "cost_histories = np.zeros((n_runs, niter))\n",
    "error_train_histories = np.zeros((n_runs, niter))\n",
    "\n",
    "# For test error calculation (for reasons of time cost not for all iterations)\n",
    "iters_selected = [(i+1)*10-1 for i in range(int(niter/10))]\n",
    "iters_selected.insert(0, 0)\n",
    "error_test_histories = np.zeros((n_runs, len(iters_selected)))\n",
    "\n",
    "for i in range(n_runs):\n",
    "    print('---------------------------------')\n",
    "    print('Start run ', i+1)\n",
    "    rand_unshaped_pars = randparams(par_shapes)\n",
    "    start = time()\n",
    "    res = my_spsa(get_probs, get_cost, get_train_error, get_test_error, rand_unshaped_pars,\n",
    "                  bounds=bounds, niter=niter, a=a_est, c=c_fix,\n",
    "                  print_iter=True, correct_func_value=False, filename=('MC_task_SPSAOutput_Run' + str(i)),\n",
    "                  iters_selected=iters_selected)\n",
    "    param_histories.append(res[0])   \n",
    "    cost_histories[i, :] = res[1]\n",
    "    error_train_histories[i, :] = res[2]\n",
    "    error_test_histories[i, :] = res[3]\n",
    "    print('run', i+1, 'done')\n",
    "    print('Time taken: ', time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25ff68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#****************************************************\n",
    "# Averaging\n",
    "#****************************************************\n",
    "\n",
    "# In case N_runs > 1, one may want to calculate cost and errors averaged over several runs...\n",
    "\n",
    "# In this example notebook however not done, hence:\n",
    "cost_history = cost_histories[0, :]\n",
    "error_train_history = error_train_histories[0, :]\n",
    "error_test_history = error_test_histories[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334c5fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#****************************************************\n",
    "# Summary plot\n",
    "#****************************************************\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(13, 8))\n",
    "\n",
    "ax1.plot(range(len(cost_history)), cost_history, '-k', markersize=4, label='cost')\n",
    "ax1.set_ylabel(r\"Cost\", fontsize='x-large')\n",
    "ax1.set_xlabel(r\"SPSA~iterations\", fontsize='x-large')\n",
    "ax1.legend(loc='upper center', fontsize='x-large')\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.set_ylabel(r\"Error in \\%\", fontsize='x-large')\n",
    "ax2.plot(range(len(error_train_history)), error_train_history, '-g', markersize=4, label='training error')\n",
    "ax2.plot(iters_selected, error_test_history, 'xb', markersize=7, label='testing error')\n",
    "ax2.legend(loc='upper right', fontsize='x-large')\n",
    "\n",
    "plt.title('MC task, quantum run -- results', fontsize='x-large')\n",
    "plt.savefig('MC_task_Results.png', dpi=300, facecolor='white')  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf027d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
