{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f23ec3",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#*****************************************************************************************************\n",
    "# Fix settings concerning the ansatz, optimisation and backend\n",
    "#*****************************************************************************************************\n",
    "from time import time \n",
    "import pickle \n",
    "import numpy as np\n",
    "\n",
    "from discopy import Ty, Id, Box, Diagram, Word\n",
    "from discopy.rigid import Cup, Cap, Functor, Swap\n",
    "from discopy.quantum.circuit import bit, qubit\n",
    "from discopy.quantum import Measure\n",
    "from discopy.quantum.tk import to_tk\n",
    "from discopy.quantum.tk import Circuit as tk_Circuit_inDCP\n",
    "\n",
    "from pytket import Circuit as tk_Circuit\n",
    "\n",
    "#-----------------------------\n",
    "# atomic pregroup grammar types\n",
    "#-----------------------------\n",
    "s, n = Ty('S'), Ty('N')\n",
    "\n",
    "#----------------------------------------\n",
    "# settings concerning the ansaetze\n",
    "#----------------------------------------\n",
    "q_s = 1        # number of qubits for sentence type s\n",
    "q_n = 1        # number of qubits for noun type n\n",
    "depth = 1      # number of IQP layers for non-single-qubit words\n",
    "p_n = 3        # number of parameters for a single-qubit word (noun); valued in {1,2,3}.\n",
    "\n",
    "#----------------------------------------\n",
    "# Parameters concerning the optimisation\n",
    "#----------------------------------------\n",
    "n_runs = 20       # number of runs over training procedure\n",
    "niter  = 2000     # number of iterations for any optimisation run of training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018ba709",
   "metadata": {},
   "outputs": [],
   "source": [
    "#******************************************\n",
    "# Data import\n",
    "#******************************************\n",
    "\n",
    "# import train and test datasets: the data entries are all strings of the form 'label sentence' \n",
    "# with the label in {0,1} and with the sentence of the form \"word1_POStag1 word2_POStag2 ...\"\n",
    "\n",
    "with open('../datasets/mc_train_data.txt') as f:\n",
    "    training_data_raw = f.readlines()\n",
    "    \n",
    "with open('../datasets/mc_dev_data.txt') as f:\n",
    "    dev_data_raw = f.readlines()\n",
    "\n",
    "with open('../datasets/mc_test_data.txt') as f:\n",
    "    testing_data_raw = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd75b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#***************************************************************\n",
    "# Turn the raw input data into data structures convenient below\n",
    "#***************************************************************\n",
    "\n",
    "vocab = dict()          # dictionary to be filled with the vocabulary in the form { word : POStag }\n",
    "data = dict()           # dictionary to be filled with all the data (train, dev and test subsets); entries of the \n",
    "                        # form { sentence : label } with label encoding '1' as [1.0, 0.0] and '0' as [0.0, 1.0]\n",
    "training_data = []      # list of sentences in the train dataset as strings \"word1 word2 ...\"\n",
    "dev_data = []           # list of sentences in the dev dataset as strings \"word1 word2 ...\"\n",
    "testing_data = []       # list of sentences in the test dataset as strings \"word1 word2 ...\"\n",
    "\n",
    "# Go through the train data\n",
    "for sent in training_data_raw:\n",
    "    words = sent[2:].split() \n",
    "    sent_untagged = ''\n",
    "    for word in words:\n",
    "        word_untagged, tag = word.split('_')\n",
    "        vocab[word_untagged] = tag\n",
    "        sent_untagged += word_untagged + ' '\n",
    "    sentence = sent_untagged[:-1]\n",
    "    training_data.append(sentence)\n",
    "    label = np.array([1.0, 0.0]) if sent[0] == '1' else np.array([0.0, 1.0])\n",
    "    data[sentence] = label\n",
    "\n",
    "# Go through the dev data\n",
    "for sent in dev_data_raw:\n",
    "    words = sent[2:].split() \n",
    "    sent_untagged = ''\n",
    "    for word in words:\n",
    "        word_untagged, tag = word.split('_')\n",
    "        vocab[word_untagged] = tag\n",
    "        sent_untagged += word_untagged + ' '\n",
    "    sentence = sent_untagged[:-1]\n",
    "    dev_data.append(sentence)\n",
    "    label = np.array([1.0, 0.0]) if sent[0] == '1' else np.array([0.0, 1.0])\n",
    "    data[sentence] = label\n",
    "    \n",
    "# Go through the test data\n",
    "for sent in testing_data_raw:\n",
    "    words = sent[2:].split() \n",
    "    sent_untagged = ''\n",
    "    for word in words:\n",
    "        word_untagged, tag = word.split('_')\n",
    "        vocab[word_untagged] = tag\n",
    "        sent_untagged += word_untagged + ' '\n",
    "    sentence = sent_untagged[:-1]\n",
    "    testing_data.append(sentence)\n",
    "    label = np.array([1.0, 0.0]) if sent[0] == '1' else np.array([0.0, 1.0])\n",
    "    data[sentence] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cf05d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#*****************************************************\n",
    "# The sentences as diagrams via CFG production rules\n",
    "#*****************************************************\n",
    "\n",
    "#----------------------------\n",
    "# Further POS tags:\n",
    "#----------------------------\n",
    "nphr, adj, tv, iv, vphr = Ty('NP'), Ty('ADJ'), Ty('TV'), Ty('IV'), Ty('VP')\n",
    "\n",
    "#----------------------------\n",
    "# The vocabulary in DisCoPy\n",
    "#----------------------------\n",
    "vocab_dict_boxes = dict()\n",
    "for word, tag in vocab.items():\n",
    "    if tag == 'N':\n",
    "        vocab_dict_boxes.update({word: Word(word, n)})\n",
    "    if tag == 'TV':\n",
    "        vocab_dict_boxes.update({word: Word(word, tv)})\n",
    "    if tag == 'ADJ':\n",
    "        vocab_dict_boxes.update({word: Word(word, adj)})\n",
    "\n",
    "#-------------------------------------\n",
    "# The CFG production rules as boxes\n",
    "#-------------------------------------\n",
    "r0 = Box('R0', nphr @ vphr, s)\n",
    "r1 = Box('R1', tv @ nphr, vphr)\n",
    "r2 = Box('R2', adj @ n, nphr)\n",
    "r3 = Box('R3', iv, vphr)\n",
    "r4 = Box('R4', n, nphr)\n",
    "\n",
    "#---------------------------------------------\n",
    "# The needed grammatical sentence structures\n",
    "#---------------------------------------------\n",
    "grammar_dict = {\n",
    "    'N_TV_N': ((Id(n @ tv) @ r4) >> (r4 @ r1) >> r0),\n",
    "    'N_TV_ADJ_N': ((Id(n @ tv) @ r2) >> (r4 @ r1) >> r0),\n",
    "    'ADJ_N_TV_N': ((Id(adj @ n @ tv) @ r4) >> (r2 @ r1) >> r0),\n",
    "}\n",
    "\n",
    "#---------------------------------------------\n",
    "# Create CFG diagrams for the sentences\n",
    "#---------------------------------------------\n",
    "sentences_dict = dict()\n",
    "for sentstr in list(data.keys()):\n",
    "    grammar_id = ''\n",
    "    sentence = Id(Ty())\n",
    "    for word in sentstr.split(' '):\n",
    "        grammar_id += (vocab[word] + '_')\n",
    "        sentence = sentence @ vocab_dict_boxes[word]\n",
    "    grammar_id = grammar_id[:-1]\n",
    "    sentence = sentence >> grammar_dict[grammar_id]\n",
    "    sentences_dict.update({sentstr: [sentence, grammar_id]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e9372c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#***************************************************************\n",
    "# Translation to pregroup grammar \n",
    "#***************************************************************\n",
    "from discopy.grammar.pregroup import draw\n",
    "\n",
    "# From POS tags to Pregroup types:\n",
    "ob_pg = {n: n, s: s, adj: n @ n.l, tv: n.r @ s @ n.l, vphr:  n.r @ s, nphr: n}\n",
    "\n",
    "# From CFG rules to Pregroup reductions: \n",
    "ar_pg = {\n",
    "    r0: Cup(n, n.r) @ Id(s),\n",
    "    r1: Id(n.r @ s) @ Cup(n.l, n),\n",
    "    r2: Id(n) @ Cup(n.l, n),\n",
    "    r3: Id(n.r @ s),\n",
    "    r4: Id(n)\n",
    "}\n",
    "\n",
    "# The vocabulary as DisCoPy boxes with pregroup types\n",
    "vocab_pg = [Word(vocab_dict_boxes[word].name, ob_pg[vocab_dict_boxes[word].cod]) for word in vocab.keys()]\n",
    "\n",
    "# The mapping of morphisms\n",
    "ar_pg.update({vocab_dict_boxes[word]: Word(vocab_dict_boxes[word].name, ob_pg[vocab_dict_boxes[word].cod]) for word in vocab.keys()})\n",
    "\n",
    "# The functor that translates from CFG to pregroup\n",
    "t2p = Functor(ob_pg, ar_pg)\n",
    "\n",
    "sentences_pg_dict = dict()\n",
    "for sentstr in sentences_dict:\n",
    "    sentences_pg_dict.update({sentstr: [t2p(sentences_dict[sentstr][0]), sentences_dict[sentstr][1]]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc90cd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#******************************************************************************************************\n",
    "# (Optional) For visualisation: the sentences as pregroup diagrams -- before 'bending nouns around'\n",
    "#******************************************************************************************************\n",
    "\n",
    "for sentstr in sentences_pg_dict:\n",
    "    sentences_pg_dict[sentstr][0].draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672d2e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#******************************************************\n",
    "# Bending the nouns around\n",
    "#******************************************************\n",
    "sentences_pg_psr_dict = dict()\n",
    "\n",
    "for sentstr in sentences_pg_dict:\n",
    "    grammar_id = sentences_pg_dict[sentstr][1]\n",
    "    num_words = len(grammar_id.split('_'))\n",
    "    words = sentences_pg_dict[sentstr][0][:num_words].boxes\n",
    "    grammar = sentences_pg_dict[sentstr][0][num_words:]\n",
    "    if grammar_id == 'N_TV_N':\n",
    "        noun1 = Box(words[0].name, n.r, Ty())\n",
    "        noun2 = Box(words[2].name, n.l, Ty())\n",
    "        words_new = (Cap(n.r, n) @ Cap(n, n.l)) >> (noun1 @ Id(n) @ words[1] @ Id(n) @ noun2)\n",
    "    if grammar_id == 'ADJ_N_TV_N':\n",
    "        noun1 = Box(words[1].name, n.l, Ty())\n",
    "        noun2 = Box(words[3].name, n.l, Ty())\n",
    "        words_new = (Cap(n, n.l) @ Cap(n, n.l)) >> (words[0] @ Id(n) @ noun1 @ words[2] @ Id(n) @ noun2)\n",
    "    if grammar_id == 'N_TV_ADJ_N':\n",
    "        noun1 = Box(words[0].name, n.r, Ty())\n",
    "        noun2 = Box(words[3].name, n.l, Ty())\n",
    "        words_new = (Cap(n.r, n) @ Cap(n, n.l)) >> (noun1 @ Id(n) @ words[1] @ words[2] @ Id(n) @ noun2)\n",
    "    # add newly wired sentence to dictionary\n",
    "    sentence = words_new >> grammar\n",
    "    # Yank snakes and add to dictionary\n",
    "    sentences_pg_psr_dict.update({sentstr: sentence.normal_form()})\n",
    "\n",
    "# Now for the vocab\n",
    "vocab_psr = []\n",
    "for word in vocab_pg:\n",
    "    if word.cod == Ty('N'):\n",
    "        vocab_psr.append(Box(word.name, n.r, Ty()))   # n.l case is dealt with in definition of quantum functor\n",
    "    else:\n",
    "        vocab_psr.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1d81a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#******************************************************************************************************\n",
    "# (Optional) For visualisation: the sentences as pregroup diagrams -- after 'bending nouns around'\n",
    "#******************************************************************************************************\n",
    "for sentstr in sentences_pg_psr_dict:\n",
    "    sentences_pg_psr_dict[sentstr].draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78dca105",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#*****************************************************\n",
    "# Translation to quantum circuits\n",
    "#*****************************************************\n",
    "from discopy.quantum import Ket, IQPansatz, Bra\n",
    "from discopy.quantum.gates import sqrt, H, CZ, Rz, Rx, CX\n",
    "from discopy.quantum.circuit import Id\n",
    "from discopy import CircuitFunctor\n",
    "from discopy.quantum.circuit import Circuit as DCP_Circuit\n",
    "\n",
    "ob = {s: q_s, n: q_n}                           # assignment of number of qubits to atomic grammatical types\n",
    "ob_cqmap = {s: qubit ** q_s, n: qubit ** q_n}   # the form in which it is needed for discopy's cqmap module\n",
    "\n",
    "#-----------------------------------------\n",
    "# parametrised part of ansaetze\n",
    "#-----------------------------------------\n",
    "\n",
    "def single_qubit_iqp_ansatz(params):\n",
    "    if len(params) == 1:\n",
    "        return Rx(params[0])  \n",
    "    if len(params) == 2:\n",
    "        return Rx(params[0]) >> Rz(params[1])\n",
    "    if len(params) == 3:\n",
    "        return IQPansatz(1, params)       \n",
    "\n",
    "def ansatz_state(state, params):  \n",
    "    arity = sum(ob[Ty(factor.name)] for factor in state.cod)\n",
    "    if arity == 1:\n",
    "        return Ket(0) >> single_qubit_iqp_ansatz(params)\n",
    "    else:\n",
    "        return Ket(*tuple([0 for i in range(arity)])) >> IQPansatz(arity, params)\n",
    "    \n",
    "def ansatz_effect(effect, params):  \n",
    "    arity = sum(ob[Ty(factor.name)] for factor in effect.dom)\n",
    "    if arity == 1:\n",
    "        return single_qubit_iqp_ansatz(params) >> Bra(0)\n",
    "    else:\n",
    "        return IQPansatz(arity, params) >> Bra(*tuple([0 for i in range(arity)]))\n",
    "       \n",
    "def ansatz(box, params):\n",
    "    dom_type = box.dom\n",
    "    cod_type = box.cod\n",
    "    if len(dom_type) == 0 and len(cod_type) != 0:\n",
    "        return ansatz_state(box, params)\n",
    "    if len(dom_type) != 0 and len(cod_type) == 0:\n",
    "        return ansatz_effect(box, params)\n",
    "\n",
    "#----------------------------------------------------------\n",
    "# Define parametrised functor to quantum circuits\n",
    "#----------------------------------------------------------\n",
    "\n",
    "def F(params): \n",
    "    ar = dict()\n",
    "    for i in range(len(vocab_psr)):\n",
    "        pgbox = vocab_psr[i]\n",
    "        qbox = ansatz(vocab_psr[i], params[i])\n",
    "        ar.update({pgbox: qbox})\n",
    "        if pgbox.cod == Ty():\n",
    "            ar.update({Box(pgbox.name, n.l, Ty()): qbox})\n",
    "    return CircuitFunctor(ob_cqmap, ar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f439b8",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#*****************************************************\n",
    "# The functions to deal with the parametrisation\n",
    "#*****************************************************\n",
    "\n",
    "def param_shapes(vocab_psr):\n",
    "    parshapes = []    \n",
    "    for box in vocab_psr:\n",
    "        dom_type = box.dom\n",
    "        cod_type = box.cod\n",
    "        dom_arity = sum(ob[Ty(factor.name)] for factor in box.dom)\n",
    "        cod_arity = sum(ob[Ty(factor.name)] for factor in box.cod)\n",
    "        if dom_arity == 0 or cod_arity == 0:  # states and effects\n",
    "            arity = max(dom_arity, cod_arity)\n",
    "            assert arity != 0\n",
    "            if arity == 1:\n",
    "                parshapes.append((p_n,))       \n",
    "            if arity != 1:\n",
    "                parshapes.append((depth, arity-1))\n",
    "    return parshapes\n",
    "\n",
    "def rand_params(par_shapes):\n",
    "    params = np.array([]) \n",
    "    for i in range(len(par_shapes)):\n",
    "         params = np.concatenate((params, np.ravel(np.random.rand(*par_shapes[i]))))\n",
    "    return params \n",
    "\n",
    "def reshape_params(unshaped_pars, par_shapes):\n",
    "    pars_reshaped = [[] for ii in range(len(par_shapes))]\n",
    "    shift = 0\n",
    "    for ss, s in enumerate(par_shapes):\n",
    "        idx0 = 0 + shift\n",
    "        if len(s) == 1:\n",
    "            idx1 = s[0] + shift\n",
    "        elif len(s) == 2:\n",
    "            idx1 = s[0] * s[1] + shift\n",
    "        pars_reshaped[ss] = np.reshape(unshaped_pars[idx0:idx1], s)\n",
    "        if len(s) == 1:\n",
    "            shift += s[0]\n",
    "        elif len(s) == 2:\n",
    "            shift += s[0] * s[1]\n",
    "    return pars_reshaped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4f8555",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#****************************************\n",
    "# The parameters of the current model\n",
    "#****************************************\n",
    "\n",
    "par_shapes = param_shapes(vocab_psr)\n",
    "rand_unshaped_pars = rand_params(par_shapes)\n",
    "rand_shaped_pars = reshape_params(rand_unshaped_pars, par_shapes)\n",
    "\n",
    "print('Number of parameters:    ', len(rand_unshaped_pars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2817614",
   "metadata": {},
   "outputs": [],
   "source": [
    "#**************************************************************\n",
    "# (Optional) Quantum circuit diagrams for the sentences \n",
    "#**************************************************************\n",
    "\n",
    "func = F(rand_shaped_pars)\n",
    "\n",
    "for sentstr in sentences_pg_psr_dict:\n",
    "    func(sentences_pg_psr_dict[sentstr]).draw(draw_box_labels=True, figsize=(5, 5), nodesize = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf5f671",
   "metadata": {},
   "outputs": [],
   "source": [
    "#********************************************************************************************\n",
    "# Encode data such that the circuits (for one call of cost function etc.) can be sent as one\n",
    "# job to quantum hardware.\n",
    "#********************************************************************************************\n",
    "\n",
    "train_labels = []\n",
    "train_circuits_pg_psr = []\n",
    "for sentstr in training_data:\n",
    "    train_circuits_pg_psr.append(sentences_pg_psr_dict[sentstr])\n",
    "    train_labels.append(np.array(data[sentstr]))\n",
    "train_labels = np.array(train_labels)\n",
    "\n",
    "dev_labels = []\n",
    "dev_circuits_pg_psr = []\n",
    "for sentstr in dev_data:\n",
    "    dev_circuits_pg_psr.append(sentences_pg_psr_dict[sentstr])\n",
    "    dev_labels.append(np.array(data[sentstr]))\n",
    "dev_labels = np.array(dev_labels)\n",
    "\n",
    "test_labels = []\n",
    "test_circuits_pg_psr = []\n",
    "for sentstr in testing_data:\n",
    "    test_circuits_pg_psr.append(sentences_pg_psr_dict[sentstr])\n",
    "    test_labels.append(np.array(data[sentstr]))\n",
    "test_labels = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7c1b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#**********************************************************************************************************\n",
    "# The cost function for optimisation and the error functions \n",
    "#**********************************************************************************************************\n",
    "from jax import numpy as jnp\n",
    "from jax import jit\n",
    "\n",
    "def get_cost(unshaped_params):\n",
    "    func = F(reshape_params(unshaped_params, par_shapes))\n",
    "    train_circuits = [func(circ) for circ in train_circuits_pg_psr]\n",
    "    results = DCP_Circuit.eval(*train_circuits)\n",
    "    results_tweaked = [jnp.abs(jnp.array(res.array) - 1e-9) for res in results]\n",
    "    pred_labels_distrs = [res / jnp.sum(res) for res in results_tweaked]\n",
    "    cross_entropies = jnp.array([jnp.sum(train_labels[s] * jnp.log2(pred_labels_distrs[s])) for s in range(len(train_labels))])\n",
    "    return -1 / len(training_data) * jnp.sum(cross_entropies)\n",
    "\n",
    "def get_train_error(unshaped_params):\n",
    "    func = F(reshape_params(unshaped_params, par_shapes))\n",
    "    train_circuits = [func(circ) for circ in train_circuits_pg_psr]\n",
    "    results = DCP_Circuit.eval(*train_circuits)\n",
    "    results_tweaked = [jnp.abs(jnp.array(res.array) - 1e-9) for res in results]\n",
    "    pred_labels_distrs = [res / jnp.sum(res) for res in results_tweaked]\n",
    "    assert len(pred_labels_distrs[0]) == 2  # rounding only makes sense if labels are binary tuples\n",
    "    pred_labels = [jnp.round(res) for res in pred_labels_distrs]\n",
    "    error = 0.0\n",
    "    for i in range(len(pred_labels)):\n",
    "        diff = jnp.sum(jnp.abs(train_labels[i] - pred_labels[i]))\n",
    "        error += jnp.min(jnp.array([diff, 1.0]))\n",
    "    return error * 100 / len(training_data)\n",
    "\n",
    "def get_dev_error(unshaped_params):\n",
    "    func = F(reshape_params(unshaped_params, par_shapes))\n",
    "    dev_circuits = [func(circ) for circ in dev_circuits_pg_psr]\n",
    "    results = DCP_Circuit.eval(*dev_circuits)\n",
    "    results_tweaked = [jnp.abs(jnp.array(res.array) - 1e-9) for res in results]\n",
    "    pred_labels_distrs = [res / jnp.sum(res) for res in results_tweaked]\n",
    "    assert len(pred_labels_distrs[0]) == 2  # rounding only makes sense if labels are binary tuples\n",
    "    pred_labels = [jnp.round(res) for res in pred_labels_distrs]\n",
    "    error = 0.0\n",
    "    for i in range(len(pred_labels)):\n",
    "        diff = jnp.sum(jnp.abs(dev_labels[i] - pred_labels[i]))\n",
    "        error += jnp.min(jnp.array([diff, 1.0]))\n",
    "    return error * 100 / len(dev_data)\n",
    "\n",
    "\n",
    "def get_test_error(unshaped_params):\n",
    "    func = F(reshape_params(unshaped_params, par_shapes))\n",
    "    test_circuits = [func(circ) for circ in test_circuits_pg_psr]\n",
    "    results = DCP_Circuit.eval(*test_circuits)\n",
    "    results_tweaked = [jnp.abs(jnp.array(res.array) - 1e-9) for res in results]\n",
    "    pred_labels_distrs = [res / jnp.sum(res) for res in results_tweaked]\n",
    "    assert len(pred_labels_distrs[0]) == 2  # rounding only makes sense if labels are binary tuples\n",
    "    pred_labels = [jnp.round(res) for res in pred_labels_distrs]\n",
    "    error = 0.0\n",
    "    for i in range(len(pred_labels)):\n",
    "        diff = jnp.sum(jnp.abs(test_labels[i] - pred_labels[i]))\n",
    "        error += jnp.min(jnp.array([diff, 1.0]))\n",
    "    return error * 100 / len(testing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cc2d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#*************************************************************************************************\n",
    "# Define the jitted versions of above three functions. Then one by one do a \n",
    "# loop over two calls to let jit do its thing so that function call is fast when doing optimisation.\n",
    "#*************************************************************************************************\n",
    "\n",
    "get_cost_jit = jit(get_cost)\n",
    "get_train_error_jit = jit(get_train_error)\n",
    "get_dev_error_jit = jit(get_dev_error)\n",
    "get_test_error_jit = jit(get_test_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2767339e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2):\n",
    "    rand_unshaped_pars = rand_params(par_shapes)\n",
    "    print('-------------')\n",
    "    start = time()\n",
    "    print('Cost: ', get_cost_jit(rand_unshaped_pars))\n",
    "    print('Time taken for this iteration: ', time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a182a6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2):\n",
    "    rand_unshaped_pars = rand_params(par_shapes)\n",
    "    print('-------------')\n",
    "    start = time()\n",
    "    print('Train Error: ', get_train_error_jit(rand_unshaped_pars))\n",
    "    print('Time taken for this iteration: ', time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c041fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2):\n",
    "    rand_unshaped_pars = rand_params(par_shapes)\n",
    "    print('-------------')\n",
    "    start = time()\n",
    "    print('Dev Error: ', get_dev_error_jit(rand_unshaped_pars))\n",
    "    print('Time taken for this iteration: ', time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84323f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2):\n",
    "    rand_unshaped_pars = rand_params(par_shapes)\n",
    "    print('-------------')\n",
    "    start = time()\n",
    "    print('Test Error: ', get_test_error_jit(rand_unshaped_pars))\n",
    "    print('Time taken for this iteration: ', time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693d6df8",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#**********************************************************************************\n",
    "# Minimization algorithm\n",
    "#**********************************************************************************\n",
    "\n",
    "# This is building on the minimizeSPSA function from the noisyopt package (https://github.com/andim/noisyopt);\n",
    "# here only adjusted for our purposes (mostly with quantum implementations in mind)\n",
    "\n",
    "def my_spsa(get_cost, get_train_error, x0,\n",
    "            bounds=None, niter=100, a=1.0, c=1.0, alpha=0.602, gamma=0.101,\n",
    "            print_iter=False, filename='spsa_output'):\n",
    "    A = 0.01 * niter\n",
    "    N = len(x0)\n",
    "    if bounds is None:\n",
    "        project = lambda x: x\n",
    "    else:\n",
    "        bounds = np.asarray(bounds)\n",
    "        project = lambda x: np.clip(x, bounds[:, 0], bounds[:, 1])    \n",
    "    param_history = []\n",
    "    func_history = []\n",
    "    error_history = []\n",
    "    x = x0    \n",
    "    \n",
    "    # Loop over iterations\n",
    "    for k in range(niter):\n",
    "        if print_iter:\n",
    "            print('-------------', '\\n', 'iteration: ', k, sep='')\n",
    "        start = time()\n",
    "        \n",
    "        # determine stepping parameters\n",
    "        ak = a/(k+1.0+A)**alpha\n",
    "        ck = c/(k+1.0)**gamma\n",
    "        delta = np.random.choice([-1, 1], size=N)\n",
    "        \n",
    "        # move in + direction from previous x\n",
    "        xplus = project(x + ck*delta)        \n",
    "        if print_iter:\n",
    "            print('Call for xplus')\n",
    "        funcplus = get_cost(xplus)\n",
    "        \n",
    "        # move in - direction from previous x\n",
    "        xminus = project(x - ck * delta)\n",
    "        if print_iter:\n",
    "            print('Call for xminus')\n",
    "        funcminus = get_cost(xminus)\n",
    "        \n",
    "        # new step\n",
    "        grad = (funcplus - funcminus) / (xplus-xminus)\n",
    "        x = project(x - ak*grad)\n",
    "        param_history.append(x)\n",
    "        \n",
    "        # determine current func and error\n",
    "        current_func_value = get_cost(x)\n",
    "        error = get_train_error(x)\n",
    "        func_history.append(current_func_value)\n",
    "        error_history.append(error)\n",
    "\n",
    "        # save to file\n",
    "        dump_data = {\n",
    "            'param_history': param_history,\n",
    "            'func_history': func_history,\n",
    "            'error_history': error_history\n",
    "        }\n",
    "        with open(filename+'.pickle', 'wb') as file_handle:\n",
    "            pickle.dump(dump_data, file_handle)\n",
    "        \n",
    "        if print_iter:\n",
    "            print('Time taken for this iteration: ', time() - start)\n",
    "    return param_history, func_history, error_history "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d114d175",
   "metadata": {},
   "outputs": [],
   "source": [
    "#*****************************************************\n",
    "# Optimisation settings\n",
    "#*****************************************************\n",
    "\n",
    "bounds = [[0.0, 1.0] for ii in range(len(rand_unshaped_pars))]\n",
    "\n",
    "c_fix = 0.1  \n",
    "#leave alpha and gamma as the default (which is as recommended), i.e. alpha = 0.602 and gamma = 0.101\n",
    "\n",
    "#-----------------------------------\n",
    "# calculate well-educated guess for parameter 'a'. \n",
    "# (below calcucation follows the heuristics from: \n",
    "#  https://www.jhuapl.edu/SPSA/PDF-SPSA/Spall_Implementation_of_the_Simultaneous.PDF )\n",
    "#-----------------------------------\n",
    "\n",
    "desired_param_change = 0.005  # rough change of a parameter in early iterations.\n",
    "\n",
    "alpha = 0.602     \n",
    "A = niter*0.01    \n",
    "c_0 = c_fix\n",
    "\n",
    "a_est = 0.0\n",
    "nruns = 1000\n",
    "for l in range(nruns):\n",
    "    rand_unshaped_pars = rand_params(par_shapes)\n",
    "    delta_0 = np.array([np.random.randint(0,2) for i in range(len(rand_unshaped_pars))])*2\\\n",
    "              - np.array([1 for i in range(len(rand_unshaped_pars))])\n",
    "    g0_estimate = (get_cost_jit(rand_unshaped_pars + c_0*delta_0) - get_cost_jit(rand_unshaped_pars - c_0*delta_0))/(2*c_0)\n",
    "    a_est += np.abs(desired_param_change * ((A +1)**alpha) / g0_estimate )\n",
    "a_est = a_est/nruns\n",
    "print('Calculated good choice for a=', a_est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed0009d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#**********************************************************\n",
    "# Training the model\n",
    "#**********************************************************\n",
    "param_histories = []\n",
    "cost_histories = np.zeros((n_runs, niter))\n",
    "error_train_histories = np.zeros((n_runs, niter))\n",
    "\n",
    "for i in range(n_runs):\n",
    "    print('---------------------------------')\n",
    "    print('Start run ', i+1)\n",
    "    rand_unshaped_pars = rand_params(par_shapes)\n",
    "    start = time()\n",
    "    res = my_spsa(get_cost_jit, get_train_error_jit, rand_unshaped_pars,\n",
    "                  bounds=bounds, niter=niter, a=a_est, c=c_fix,\n",
    "                  print_iter=False, filename=('RP_task_SPSAOutput_ECS_Run' + str(i)))\n",
    "    param_histories.append(res[0])   \n",
    "    cost_histories[i, :] = res[1]\n",
    "    error_train_histories[i, :] = res[2]    \n",
    "    print('run', i+1, 'done')\n",
    "    print('Time taken: ', round(time() - start,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69db0bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#************************************\n",
    "# Calculate dev errors\n",
    "#************************************\n",
    "error_dev_histories = np.zeros((n_runs,niter))\n",
    "\n",
    "for i in range(n_runs):\n",
    "    dev_errors = []\n",
    "    for params in param_histories[i]:\n",
    "        dev_errors.append(get_dev_error_jit(params))\n",
    "    error_dev_histories[i,:] = dev_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0b9379",
   "metadata": {},
   "outputs": [],
   "source": [
    "#************************************\n",
    "# Calculate test errors\n",
    "#************************************\n",
    "error_test_histories = np.zeros((n_runs,niter))\n",
    "\n",
    "for i in range(n_runs):\n",
    "    test_errors = []\n",
    "    for params in param_histories[i]:\n",
    "        test_errors.append(get_test_error_jit(params))\n",
    "    error_test_histories[i,:] = test_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06647b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#************************************\n",
    "# Calculate average cost and errors\n",
    "#************************************\n",
    "\n",
    "cost_history_mean = np.zeros(niter)\n",
    "error_train_history_mean = np.zeros(niter)\n",
    "error_dev_history_mean = np.zeros(niter)\n",
    "error_test_history_mean = np.zeros(niter)\n",
    "\n",
    "for i in range(niter):\n",
    "    cost_history_mean[i] = np.mean(cost_histories[:,i])\n",
    "    error_train_history_mean[i] = np.mean(error_train_histories[:,i]) \n",
    "    error_dev_history_mean[i] = np.mean(error_dev_histories[:,i]) \n",
    "    error_test_history_mean[i] = np.mean(error_test_histories[:,i]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67b7611",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#****************************************************\n",
    "# Summary plot\n",
    "#****************************************************\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.rcParams.update({\"text.usetex\": True})\n",
    "fig, ax1 = plt.subplots(figsize=(13, 8))\n",
    "\n",
    "\n",
    "ax1.plot(range(len(cost_history_mean)), cost_history_mean, '-k', markersize=4, label='cost')\n",
    "ax1.set_ylabel(r\"Cost\", fontsize='x-large')\n",
    "ax1.set_xlabel(r\"SPSA~iterations\", fontsize='x-large')\n",
    "ax1.legend(loc='upper center', fontsize='x-large')\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.set_ylabel(r\"Error in \\%\", fontsize='x-large')\n",
    "ax2.plot(range(len(error_train_history_mean)), error_train_history_mean, '-g', markersize=4, label='train error')\n",
    "ax2.plot(range(len(error_dev_history_mean)), error_dev_history_mean, '-b', markersize=4, label='dev error')\n",
    "ax2.plot(range(len(error_test_history_mean)), error_test_history_mean, '-r', markersize=4, label='test error')\n",
    "ax2.legend(loc='upper right', fontsize='x-large')\n",
    "\n",
    "\n",
    "plt.title('MC task, classical simulation -- results', fontsize='x-large')\n",
    "plt.savefig('MC_task_ECS_Results.png', dpi=300, facecolor='white')  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc764e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# coding: utf-8",
   "executable": "/usr/bin/env python",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
